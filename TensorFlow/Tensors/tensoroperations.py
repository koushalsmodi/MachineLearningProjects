# -*- coding: utf-8 -*-
"""tensorOperations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CNCIa7X-1Pra5m7hYzosNbAmpj41Jakv

keras.layers.Dense(512, activation="relu")

output = relu(dot(input, W) + b)
here there are 3 tensor operations taking place:
- dot product: input and W
- resulting matrix is added with vector b (element-wise operation)
- relu: max(0, x) (element-wise operation)

element-wise operation: operations that are applied independently to each entry in the tensors being considered
"
"""

import numpy as np
x = np.array([[12,3,-6,14,7],
             [6,-79,-3,35,1],
             [7,80,4,-36,2]
             ])
x.ndim # 2
x.shape # (3,5)

def naive_relu(x):
  assert len(x.shape) == 2

  x = x.copy()

  for i in range(x.shape[0]):
    for j in range(x.shape[1]):
      # relu: max(0, x)
      x[i,j] = max(0, x[i,j])
  return x
naive_relu(x)
"""
array([[12,  3,  0, 14,  7],
       [ 6,  0,  0, 35,  1],
       [ 7, 80,  4,  0,  2]])
"""

y = np.array([
    [1,2,3,4,-5],
    [6,7,8,-9,10],
    [-11,12,13,14,-15]
])

def naive_add(x, y):
  #assert len(x.shape) == 2
  assert x.ndim == 2
  assert x.ndim == y.ndim
  x = x.copy()

  for i in range(x.shape[0]):
    for j in range(x.shape[1]):
      x[i,j] = x[i,j] + y[i,j]
  return x

naive_add(x, y)
"""
array([[ 13,   5,  -3,  18,   2],
       [ 12, -72,   5,  26,  11],
       [ -4,  92,  17, -22, -13]])
"""

# Comparison with pre-built numpy function
import time

x = np.random.random((20,100))
y = np.random.random((20,100))

t0 = time.time()
for _ in range(1000):
  z = x+y
  z = np.maximum(z,0.)
print("Took: {0: .2f} s".format(time.time() - t0))
# Took:  0.01 s

t0 = time.time()
for _ in range(1000):
  z = naive_add(x,y)
  z = naive_relu(z)
print("Took: {0: .2f} s".format(time.time() - t0))
# Took:  2.08 s


"""
When running TensorFlow code on a GPU, element-wise operations are executed via fully vectorized CUDA implementations that can best
utilize the highly parallel GPU chip architecture

"""

"""
Broadcasting a tensor:
1) Axes (broadcast axes): added to smaller tensor to match the ndim of the larger tensor.
2) Smaller tensor is repeated alongside the new axes to match the full shape of the larger tensor
"""

import numpy as np
X = np.random.random((32,10))
y = np.random.random((10,))

y = np.expand_dims(y, axis = 0)
print(y.ndim) # 2
print(y.shape) # (1, 10)

Y = np.concatenate([y] * 32, axis = 0) # Y[i,:] == y for i in range(0,32)
print(Y.ndim) # 2
print(Y.shape) # (32,10)

y = np.random.random((10,))

def naive_add_matrix_and_vector(x,y):
  assert len(x.shape) == 2
  assert len(y.shape) == 1
  assert x.shape[1] == y.shape[0]
  x = x.copy()

  for i in range(x.shape[0]):
    for j in range(x.shape[1]):
      x[i,j] += y[j]
  return x

result = naive_add_matrix_and_vector(X,y)
print(result.shape) # (32, 10)

# Similarly using broadcasting for element-wise maximum operation of two tensors of different shapes
a = np.random.random((64,3,32,10))
b = np.random.random((32, 10))
z = np.maximum(a,b) # (64,3,32,10)
print(z.shape)

"""
Tensor Product
z = x dot y
"""

x = np.random.random((32,))
y = np.random.random((32,))
z = np.dot(x,y) # Scalar

def naive_vector_dot(x,y):
  assert len(x.shape) == 1
  assert len(y.shape) == 1
  assert x.shape[0] == y.shape[0]
  z = 0.

  for i in range(x.shape[0]):
    z += x[i] * y[i]

  return z


result = naive_vector_dot(x,y)
print(result) # Scalar: 8.442341453650048

def naive_matrix_vector_dot(x,y):
  assert len(x.shape) == 2
  assert len(y.shape) == 1
  assert x.shape[1] == y.shape[0]

  z = np.zeros(x.shape[0])

  for i in range(x.shape[0]):
    for j in range(x.shape[1]):
      z[i]+= x[i,j] * y[j]

  return z

x = np.random.random((4,5))
y = np.random.random((5,))
result = naive_matrix_vector_dot(x,y)
print(result) # [1.98054391 1.65318042 1.56464129 1.47820925]
print(result.shape) # 4

x = np.random.random((32,10))
y = np.random.random((10,5))

def naive_matrix_dot(x,y):
  assert len(x.shape) == 2
  assert len(y.shape) == 2
  assert x.shape[1] == y.shape[0]

  z = np.zeros((x.shape[0], y.shape[1]))

  for i in range(x.shape[0]):
    for j in range(y.shape[1]):
      row_x = x[i,:]
      column_y = y[:,j]
      z[i,j] += naive_vector_dot(row_x, column_y)

  return z

result = naive_matrix_dot(x,y)
print(result.shape) # (32, 5)

# Tensor reshaping
x = np.array([[0.,1.],
             [2.,3.],
             [4.,5.]])

print(x.shape) # (3, 2)
 # (60000, 28*28) -> (60000,784)

x = x.reshape((6,1))
print(x.shape) # (6, 1)
print(x)
"""
[[0.]
 [1.]
 [2.]
 [3.]
 [4.]
 [5.]]

 """
x = x.reshape((2,3))
print(x.shape) # (2,3)

print(x)

"""
[[0. 1. 2.]
 [3. 4. 5.]]
"""

# Special case of reshaping: transposition
# (300,20) -> (20, 300)

x = np.zeros((300,20))
x = np.transpose(x)
print(x.shape) # (20, 300)

