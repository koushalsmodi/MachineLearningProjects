# -*- coding: utf-8 -*-
"""tensorOperations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CNCIa7X-1Pra5m7hYzosNbAmpj41Jakv

keras.layers.Dense(512, activation="relu")

output = relu(dot(input, W) + b)
here there are 3 tensor operations taking place:
- dot product: input and W
- resulting matrix is added with vector b (element-wise operation)
- relu: max(0, x) (element-wise operation)

element-wise operation: operations that are applied independently to each entry in the tensors being considered
"
"""

import numpy as np
x = np.array([[12,3,-6,14,7],
             [6,-79,-3,35,1],
             [7,80,4,-36,2]
             ])
x.ndim # 2
x.shape # (3,5)

def naive_relu(x):
  assert len(x.shape) == 2

  x = x.copy()

  for i in range(x.shape[0]):
    for j in range(x.shape[1]):
      # relu: max(0, x)
      x[i,j] = max(0, x[i,j])
  return x
naive_relu(x)
"""
array([[12,  3,  0, 14,  7],
       [ 6,  0,  0, 35,  1],
       [ 7, 80,  4,  0,  2]])
"""

y = np.array([
    [1,2,3,4,-5],
    [6,7,8,-9,10],
    [-11,12,13,14,-15]
])

def naive_add(x, y):
  #assert len(x.shape) == 2
  assert x.ndim == 2
  assert x.ndim == y.ndim
  x = x.copy()

  for i in range(x.shape[0]):
    for j in range(x.shape[1]):
      x[i,j] = x[i,j] + y[i,j]
  return x

naive_add(x, y)
"""
array([[ 13,   5,  -3,  18,   2],
       [ 12, -72,   5,  26,  11],
       [ -4,  92,  17, -22, -13]])
"""

# Comparison with pre-built numpy function
import time

x = np.random.random((20,100))
y = np.random.random((20,100))

t0 = time.time()
for _ in range(1000):
  z = x+y
  z = np.maximum(z,0.)
print("Took: {0: .2f} s".format(time.time() - t0))
# Took:  0.01 s

t0 = time.time()
for _ in range(1000):
  z = naive_add(x,y)
  z = naive_relu(z)
print("Took: {0: .2f} s".format(time.time() - t0))
# Took:  2.08 s


"""
When running TensorFlow code on a GPU, element-wise operations are executed via fully vectorized CUDA implementations that can best
utilize the highly parallel GPU chip architecture

"""