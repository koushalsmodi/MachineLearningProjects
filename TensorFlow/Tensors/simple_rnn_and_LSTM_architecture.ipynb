{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "JcYi_QFdCvim"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntYnrx6VCHrf",
        "outputId": "4fece4ce-b7a4-4ae7-ba1f-620eb068ef7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 16)\n"
          ]
        }
      ],
      "source": [
        "# An RNN layer that can process sequences of any length\n",
        "num_features = 14\n",
        "inputs = keras.Input(shape = (None, num_features))\n",
        "outputs = layers.SimpleRNN(16)(inputs)\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# An RNN layer that returns only its last output step\n",
        "num_features = 14\n",
        "steps = 120\n",
        "inputs = keras.Input(shape = (steps, num_features))\n",
        "outputs = layers.SimpleRNN(16, return_sequences = False)(inputs)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jft_5ArxC8SO",
        "outputId": "2de63b48-6336-4bdd-cc8e-eaa1994d09dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# An RNN layer that returns its full output sequence\n",
        "num_features = 14\n",
        "steps = 120\n",
        "inputs = keras.Input(shape = (steps, num_features))\n",
        "outputs = layers.SimpleRNN(16, return_sequences = True)(inputs)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbeFxWTgDPm4",
        "outputId": "52874173-7a6b-43c1-a92c-c3f9b3030390"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 120, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking RNN layers\n",
        "inputs = keras.Input(shape = (steps, num_features))\n",
        "x = layers.SimpleRNN(16, return_sequences=True)(inputs)\n",
        "x = layers.SimpleRNN(16, return_sequences = True)(x)\n",
        "outputs = layers.SimpleRNN(16)(x)\n",
        "# ISSUE: vanishing gradient problem: adding layers to a network eventualy becomes untrainable\n",
        "# although it should theoretically be able ton retain at time t information about inputs seen many timesteps before,\n",
        "# such long-term dependencies provde impossible to learn from practice"
      ],
      "metadata": {
        "id": "P5H8xk0IDaYX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Better approach is to use LSTM architecture\n",
        "# (1/2)\n",
        "output_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(c_t, Vo) b)\n",
        "i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)\n",
        "f_t = activation(dot(state_t, Uf) + dot(input_t, Wf)+ bf)\n",
        "k_t = activation(dot(state_t, Uk) + dot(input_t,  Wk) + bk)\n",
        "\n",
        "# (2/2)\n",
        "c_t+1 = i_t * k_t + c_t * f_t\n",
        "\n",
        "# i_t * k_t: provide information about present\n",
        "# c_t * f_t: fraction of old memory\n",
        "\n",
        "# ALLOWS past information to be reinjected at a later time, thus fighting the vanishing-gradient problem"
      ],
      "metadata": {
        "id": "mZ74YozvFTmq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}