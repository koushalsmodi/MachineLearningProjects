{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa0a376",
   "metadata": {},
   "source": [
    "## Part 2: Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f431003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92bbb244",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/Users/koushalsmodi/Desktop/MachineLearning/MachineLearningProjects/TensorFlow/CreditRisk\"\n",
    "df = pd.read_csv(base_dir+\"/home-credit-default-risk/application_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec4be3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Data Cleaning and Processing\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"* 60)\n",
    "print(\"Data Cleaning and Processing\")\n",
    "print(\"=\"* 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37067b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_credit_data(df):\n",
    "    \"\"\" Cleaning data for use in model training:\n",
    "    1: Handling Missing Values\n",
    "    2: Handling Outliers\n",
    "    3: Remove Duplicates\n",
    "    4: Data Consistency Checks\n",
    "    \"\"\"\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    cleaning_report = []\n",
    "    \n",
    "    print(\"\\n Step 1: Handling Missing Values\")\n",
    "    \n",
    "    # I am looking at patterns for missing values\n",
    "    \n",
    "    missing_cols = df_clean.columns[df_clean.isnull().any()].tolist()\n",
    "    \n",
    "    for col in missing_cols:\n",
    "        missing = df_clean[col].isnull().sum()\n",
    "        missing_pct = 100 * missing / len(df_clean)\n",
    "        \n",
    "\n",
    "        if missing_pct > 50:\n",
    "            # Dropping columns if > 50% is missing\n",
    "            df_clean = df_clean.drop(col, axis = 1)\n",
    "            cleaning_report.append(f\"Dropped {col}: {missing_pct: .2f}% missing\")\n",
    "            \n",
    "        elif df_clean[col].dtype in ['int64', 'float64']:\n",
    "            # Imputing numerical value with median (as it is not affected by outliers)\n",
    "            median_val = df_clean[col].median()\n",
    "            df_clean[col] = df_clean[col].fillna(median_val)\n",
    "            cleaning_report.append(f\"Imputed {col} with median: {median_val:.2f}\")\n",
    "            \n",
    "        else:\n",
    "            # Mode for categorical values imputation\n",
    "            mode_val = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else 'Unknown'\n",
    "            df_clean[col] = df_clean[col].fillna(mode_val)\n",
    "            cleaning_report.append(f\"Imputed {col} with mode: {mode_val}\")\n",
    "        \n",
    "    print(f\"Processed {len(missing_cols)} columns with missing values\")\n",
    "    \n",
    "    print(\"\\n Step 2: Handling Outliers\")\n",
    "    \n",
    "    numeric_cols = df_clean.select_dtypes(include = [np.number]).columns.tolist()\n",
    "    numeric_cols = [col for col in numeric_cols if col != 'TARGET']\n",
    "    \n",
    "    outlier_summary = []\n",
    "    \n",
    "     # Taking only first few to be efficient when running\n",
    "    for col in numeric_cols[:6]:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        \n",
    "        IQR = Q3 - Q1 \n",
    "        \n",
    "        # lower bound and upper bound\n",
    "        lower_bound = Q1 - 1.5 * IQR \n",
    "        upper_bound = Q3 + 1.5 * IQR \n",
    "        \n",
    "        if col.startswith('AMT_') or col.startswith('CNT_'):\n",
    "            lower_bound = max(0, lower_bound)\n",
    "        \n",
    "        if 'AGE' in col:\n",
    "            lower_bound = max(0, lower_bound)\n",
    "            upper_bound = min(100, upper_bound)\n",
    "        \n",
    "        # Counting outliers\n",
    "        \n",
    "        outliers = ((df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)).sum()\n",
    "        \n",
    "        if outliers > 0:\n",
    "            outlier_pct = 100 * outliers / len(df_clean)\n",
    "            \n",
    "            # Capping by clipping outliers instead of removing \n",
    "            df_clean[col] = df_clean[col].clip(lower = lower_bound, upper = upper_bound)\n",
    "            \n",
    "            outlier_summary.append({\n",
    "                \"Feature\": col,\n",
    "                \"Outliers\": outliers,\n",
    "                \"Percentage\": outlier_pct,\n",
    "                \"Lower Bound\": lower_bound,\n",
    "                \"Upper Bound\": upper_bound\n",
    "            })\n",
    "            \n",
    "    if outlier_summary:\n",
    "        outlier_df = pd.DataFrame(outlier_summary)\n",
    "        print(\"\\n Outliers Detected and Capped\")\n",
    "        print(outlier_df.to_string())\n",
    "        \n",
    "    \n",
    "    print(\"\\n Step 3: Remove Duplicates\")\n",
    "    \n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean.drop_duplicates(inplace = True)\n",
    "    duplicates_removed = initial_rows - len(df_clean)\n",
    "    \n",
    "    print(f\"Duplicates removed: {duplicates_removed}\")\n",
    "    \n",
    "    print(\"\\n Step 4: Data Consistency Checks\")\n",
    "    \n",
    "    issues_found = 0\n",
    "    \n",
    "    if 'AMT_INCOME_TOTAL' in df_clean.columns:\n",
    "        # Keeping only records where income >= 0\n",
    "        invalid = (df_clean['AMT_INCOME_TOTAL'] < 0).sum()\n",
    "        \n",
    "        if invalid > 0:\n",
    "            df_clean = df_clean[df_clean['AMT_INCOME_TOTAL'] >= 0]\n",
    "            print(f\"Removed {invalid} records with invalid income\")\n",
    "            issues_found += 1 \n",
    "    \n",
    "    print(f\"\\n Total issues found and corrected: {issues_found}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Cleaning Summary\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    print(f\"\\n Original Shape: {df.shape}\")\n",
    "    print(f\"\\n Cleaned Shape: {df_clean.shape}\")\n",
    "    \n",
    "    print(f\"Rows removed: {df.shape[0] - df_clean.shape[0]} ({100 * (df.shape[0] - df_clean.shape[0])/df.shape[0]:.2f}%)\")\n",
    "    print(f\"Columns removed: {df.shape[1] - df_clean.shape[1]}\")\n",
    "        \n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2144610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step 1: Handling Missing Values\n",
      "Processed 67 columns with missing values\n",
      "\n",
      " Step 2: Handling Outliers\n",
      "\n",
      " Outliers Detected and Capped\n",
      "            Feature  Outliers  Percentage  Lower Bound  Upper Bound\n",
      "0      CNT_CHILDREN      4272    1.389219            0          2.5\n",
      "1  AMT_INCOME_TOTAL     14035    4.564064            0     337500.0\n",
      "2        AMT_CREDIT      6562    2.133907            0    1616625.0\n",
      "3       AMT_ANNUITY      7504    2.440238            0      61704.0\n",
      "4   AMT_GOODS_PRICE     14728    4.789422            0    1341000.0\n",
      "\n",
      " Step 3: Remove Duplicates\n",
      "Duplicates removed: 0\n",
      "\n",
      " Step 4: Data Consistency Checks\n",
      "\n",
      " Total issues found and corrected: 0\n",
      "\n",
      "============================================================\n",
      "Cleaning Summary\n",
      "\n",
      "============================================================\n",
      "\n",
      " Original Shape: (307511, 122)\n",
      "\n",
      " Cleaned Shape: (307511, 81)\n",
      "Rows removed: 0 (0.00%)\n",
      "Columns removed: 41\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = clean_credit_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f37c528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step 1: Handling Missing Values\n",
      "Processed 67 columns with missing values\n",
      "\n",
      " Step 2: Handling Outliers\n",
      "\n",
      " Outliers Detected and Capped\n",
      "            Feature  Outliers  Percentage  Lower Bound  Upper Bound\n",
      "0      CNT_CHILDREN      4272    1.389219            0          2.5\n",
      "1  AMT_INCOME_TOTAL     14035    4.564064            0     337500.0\n",
      "2        AMT_CREDIT      6562    2.133907            0    1616625.0\n",
      "3       AMT_ANNUITY      7504    2.440238            0      61704.0\n",
      "4   AMT_GOODS_PRICE     14728    4.789422            0    1341000.0\n",
      "\n",
      " Step 3: Remove Duplicates\n",
      "Duplicates removed: 0\n",
      "\n",
      " Step 4: Data Consistency Checks\n",
      "\n",
      " Total issues found and corrected: 0\n",
      "\n",
      "============================================================\n",
      "Cleaning Summary\n",
      "\n",
      "============================================================\n",
      "\n",
      " Original Shape: (307511, 122)\n",
      "\n",
      " Cleaned Shape: (307511, 81)\n",
      "Rows removed: 0 (0.00%)\n",
      "Columns removed: 41\n",
      "✓ Saved cleaned data to application_train_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "df_clean = clean_credit_data(df)\n",
    "\n",
    "df_clean.to_csv('application_train_cleaned.csv', index=False)\n",
    "print(\"✓ Saved cleaned data to application_train_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f70538a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next is univariate analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
