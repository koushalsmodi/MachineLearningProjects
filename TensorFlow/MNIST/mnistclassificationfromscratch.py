# -*- coding: utf-8 -*-
"""MNISTclassificationfromscratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l8T_ccdvtVPbqNIs6tesEclR_NekQsxt

MNIST digit classifcation from scratch using TensorFlow.

1) Dense Layer class</br>
2) Naive Sequential layer class</br>
3) Batch Generator class</br>
4) One training step</br>
5) Update weights</br>
6) Full training loop</br>
7) Evaluation
"""

# MNIST digit classification from scratch in TensorFlow

# Importing relevant libraries
import tensorflow as tf
import math
import numpy as np

# A Simple Dense Class

class NaiveDense:
  def __init__(self, input_size, output_size, activation):
    self.activation = activation

    # Create a matrix, W, of shape (input_size, output_size), initialized with random values
    w_shape = (input_size, output_size)
    w_initial_value = tf.random.uniform(w_shape, minval = 0, maxval = 1e-1)
    self.W = tf.Variable(w_initial_value)

    # Create a vector, b, of shape (output_size, ), initialized with zeros.
    b_shape = (output_size, )
    b_initial_value = tf.zeros(b_shape)
    self.b = tf.Variable(b_initial_value)

  # Forward pass
  def __call__(self, inputs):
    return self.activation(tf.matmul(inputs, self.W) + self.b)

  # Convenience method for retrieving the layer's weights
  @property
  def weights(self):
    return [self.W, self.b]

# A Simple Sequential Class

class NaiveSequential:
  def __init__(self, layers):
    self.layers = layers

  # Forward pass
  # let's us call: output = model(input)
  def __call__(self, inputs):
    # initial data
    x = inputs

     # feeding output of 1 layer to next
    for layer in self.layers:
      x = layer(x)

    # final result
    return x

  @property
  # property lets us use it like an attribute: model.weights, not model.weights()
  def weights(self):
    weights = []
    for layer in self.layers:
      # like [W, b] and adding to the big list
      weights+=layer.weights

    return weights

model = NaiveSequential([
    NaiveDense(input_size=28*28, output_size=512, activation=tf.nn.relu),
    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)
])

# A Batch Generator

class BatchGenerator:
  def __init__(self, images, labels, batch_size=128):
    self.index = 0
    self.images = images
    self.labels = labels
    self.batch_size = batch_size
    self.num_batches = math.ceil(len(self.images) / batch_size)

  def next(self):
    images = self.images[self.index: self.index+self.batch_size]
    labels = self.labels[self.index: self.index+self.batch_size]
    self.index += self.batch_size

    return images, labels

# Training step

def one_training_step(self, images_batch, labels_batch):
  # Run the forward pass
  with tf.GradientTape() as tape:
    predictions = model(images_batch)
    per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)
    average_loss = tf.reduce_mean(per_sample_losses)

  # Compute the gradient of the loss with regard to the weights. The output gradients is a list where each
  # entry corresponds to a weights from the model.weights list
  gradients = tape.gradient(average_loss, model.weights)

  udpate_weights(gradients, model.weights)
  # Update the weights using the gradients
  return average_loss

learning_rate = 1e-3
def udpate_weights(gradients, weights):
  for g, w in zip(gradients, weights):
    # assign_sub is the equivalent of -= of TensorFlow variables
    w.assign_sub(g*learning_rate)

# The full training loop

def fit(model, images, labels, epochs, batch_size=128):

  for epoch_counter in range(epochs):
    print(f"Epoch: {epoch_counter+1}")


    batch_generator = BatchGenerator(images, labels, batch_size=128)
    for batch_counter in range(batch_generator.num_batches):
      images_batch, labels_batch = batch_generator.next()
      loss = one_training_step(model, images_batch, labels_batch)

      if batch_counter % 100 == 0:
        print(f"loss at batch {batch_counter}: {loss:.2f}")

# Test drive

from tensorflow.keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape((60000, 28*28))
train_images = train_images.astype("float32") / 255
test_images = test_images.reshape((10000, 28*28))
test_images = test_images.astype("float32") / 255

"""Epoch: 1</br>
loss at batch 0: 7.98</br>
loss at batch 100: 2.23</br>
loss at batch 200: 2.21</br>
loss at batch 300: 2.10</br>
loss at batch 400: 2.26</br>
Epoch: 2</br>
loss at batch 0: 1.92</br>
loss at batch 100: 1.88</br>
loss at batch 200: 1.83</br>
loss at batch 300: 1.73</br>
loss at batch 400: 1.87</br>
Epoch: 3</br>
loss at batch 0: 1.60</br>
loss at batch 100: 1.59</br>
loss at batch 200: 1.51</br>
loss at batch 300: 1.44</br>
loss at batch 400: 1.55</br>
Epoch: 4</br>
loss at batch 0: 1.34</br>
loss at batch 100: 1.35</br>
loss at batch 200: 1.24</br>
loss at batch 300: 1.23</br>
loss at batch 400: 1.30</br>
Epoch: 5</br>
loss at batch 0: 1.13</br>
loss at batch 100: 1.16</br>
loss at batch 200: 1.04</br>
loss at batch 300: 1.06</br>
loss at batch 400: 1.13</br>
Epoch: 6</br>
loss at batch 0: 0.98</br>
loss at batch 100: 1.02</br>
loss at batch 200: 0.90</br>
loss at batch 300: 0.94</br>
loss at batch 400: 1.01</br>
Epoch: 7</br>
loss at batch 0: 0.87</br>
loss at batch 100: 0.92</br>
loss at batch 200: 0.80</br>
loss at batch 300: 0.85</br>
loss at batch 400: 0.92</br>
Epoch: 8</br>
loss at batch 0: 0.79</br>
loss at batch 100: 0.83</br>
loss at batch 200: 0.72</br>
loss at batch 300: 0.78</br>
loss at batch 400: 0.85</br>
Epoch: 9</br>
loss at batch 0: 0.72</br>
loss at batch 100: 0.76</br>
loss at batch 200: 0.66</br>
loss at batch 300: 0.72</br>
loss at batch 400: 0.80</br>
Epoch: 10</br>
loss at batch 0: 0.67</br>
loss at batch 100: 0.71</br>
loss at batch 200: 0.61</br>
loss at batch 300: 0.67</br>
loss at batch 400: 0.75</br>

"""

fit(model, train_images, train_labels, epochs=10, batch_size=128)

# Evaluating the model

predictions = model(test_images)
predictions = predictions.numpy()
predicted_labels = np.argmax(predictions,axis = 1)
matches = predicted_labels == test_labels
print(f"accuracy: {matches.mean(): .2f}")

"""accuracy:  0.82

"""

first_layer_weights = model.layers[0].weights
W1, b1 = first_layer_weights
print("W1 shape:", W1.shape) # W1 shape: (784, 512)
print("b1 shape:", b1.shape) # b1 shape: (512,)

second_layer_weights = model.layers[1].weights
W2, b2 = second_layer_weights
print("W2 shape:", W2.shape) # W2 shape: (512, 10)
print("b2 shape:", b2.shape) # b2 shape: (10,)